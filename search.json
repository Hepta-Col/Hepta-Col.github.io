[{"title":"[Paper] Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills","url":"/2022/05/21/turning-tables/","content":"<blockquote>\n<p>Year: 2021</p>\n<p>Paper Link: <a href=\"https://arxiv.org/abs/2107.07261\">https://arxiv.org/abs/2107.07261</a></p>\n<p>Code Link: <a href=\"https://github.com/oriyor/turning_tables\">https://github.com/oriyor/turning_tables</a></p>\n<p>Key Words: reasoning, semi-structured table</p>\n</blockquote>\n<p><img src=\"/2022/05/21/turning-tables/image-20220515170620320.png\" alt=\"image-20220515170620320\"></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>Existing LMs lack reasoning skills, and the authors tackle this problem by pretraining LMs with semi-structured tables, which are used to generate different sets containing 16 kinds of question-context-answer triplets, in order to endow LMs with various reasoning skills.</p>\n<h2 id=\"Problem-Statements\"><a href=\"#Problem-Statements\" class=\"headerlink\" title=\"Problem Statements\"></a>Problem Statements</h2><p>Large pretrained LMs have been proved to struggle in performing symbolic reasoning operations. Past work on improving reasoning skills mainly have two flavors: 1) adding specialized components for specific skills (numerical reasoning, temporal reasoning), 2) generating synthetic examples at scale (using grammars, templates, question generation models).</p>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><p>The authors argue that <strong>semi-structured tables</strong> are a valuable resource for automatic generation of training data that will endow LMs with reasoning skills. Tables can be crawled from the web, and question-context-answer (q-c-a) triplets can be generated from the tables. </p>\n<p>Examples of q-c-a:</p>\n<img src=\"/2022/05/21/turning-tables/image-20220515172904363.png\" alt=\"image-20220515172904363\" style=\"zoom: 67%;\">\n\n<p>Following is the overall architecture:</p>\n<img src=\"/2022/05/21/turning-tables/image-20220515173107157.png\" alt=\"image-20220515173107157\" style=\"zoom: 67%;\">\n\n<h3 id=\"Data-Generation\"><a href=\"#Data-Generation\" class=\"headerlink\" title=\"Data Generation\"></a>Data Generation</h3><p>The authors generate table data by crawling tables from Wikipedia, and applying <strong>16 different example generators (EGs)</strong> on each table. Each EG corresponds to a particular reasoning skill (composition, numerical comparison, etc.), and comprises a small set of question templates. Variables in the templates are filled with content from the table, and the structure of the table allows to compute the answer automatically. The context is a list of facts generated from the table that contain facts required for answering the question as well as distractor facts.</p>\n<h3 id=\"Error-driven-Sampling\"><a href=\"#Error-driven-Sampling\" class=\"headerlink\" title=\"Error-driven Sampling\"></a>Error-driven Sampling</h3><p>The training process involves learning on multiple results from different EGs, so a proper multi-task training method is required. The authors make the training focus on the reasoning skills that the model still lacks, i.e. <strong>error-driven sampling</strong>. </p>\n<p>Past work have proposed uniform sampling, error sampling, etc.. An issue with error sampling is that if the error rate is high for a task and learning it is slow, the model will spend most time on that task at the expense of all other tasks, which may lead overall to low data efficiency. The authors put forward <strong>momentum sampling</strong>, which samples examples from a task in proportion to its rate of improvement, putting most probability mass on skills that are improving quickly.</p>\n<img src=\"/2022/05/21/turning-tables/image-20220515181058227.png\" alt=\"image-20220515181058227\" style=\"zoom:67%;\">\n\n<h3 id=\"Finetune\"><a href=\"#Finetune\" class=\"headerlink\" title=\"Finetune\"></a>Finetune</h3><p>The authors finetune the Pre-trained for Reasoning Model, <strong>PReasM</strong>, on three RC datasets that require reasoning: DROP, IIRC, and MMQA. </p>\n<h2 id=\"Evaluations\"><a href=\"#Evaluations\" class=\"headerlink\" title=\"Evaluations\"></a>Evaluations</h2><h3 id=\"Experiment-Setup\"><a href=\"#Experiment-Setup\" class=\"headerlink\" title=\"Experiment Setup\"></a>Experiment Setup</h3><h4 id=\"Models\"><a href=\"#Models\" class=\"headerlink\" title=\"Models\"></a>Models</h4><ul>\n<li>baseline: T5 (T5-base, T5-large)</li>\n</ul>\n<h4 id=\"Datasets-amp-Metrics\"><a href=\"#Datasets-amp-Metrics\" class=\"headerlink\" title=\"Datasets &amp; Metrics\"></a>Datasets &amp; Metrics</h4><ul>\n<li>DROP: F1, EM</li>\n<li>IIRC: F1, EM</li>\n<li>MMQA: F1, EM</li>\n</ul>\n<h3 id=\"Results-of-Interest\"><a href=\"#Results-of-Interest\" class=\"headerlink\" title=\"Results of Interest\"></a>Results of Interest</h3><h4 id=\"Performance-on-each-dataset\"><a href=\"#Performance-on-each-dataset\" class=\"headerlink\" title=\"Performance on each dataset\"></a>Performance on each dataset</h4><img src=\"/2022/05/21/turning-tables/image-20220515181941958.png\" alt=\"image-20220515181941958\" style=\"zoom:67%;\">\n\n<h4 id=\"Effect-of-new-sampling-strategy\"><a href=\"#Effect-of-new-sampling-strategy\" class=\"headerlink\" title=\"Effect of new sampling strategy\"></a>Effect of new sampling strategy</h4><img src=\"/2022/05/21/turning-tables/image-20220515182134219.png\" alt=\"image-20220515182134219\" style=\"zoom:67%;\">","categories":["Paper Notes","NLP"],"tags":["Deep Learning","NLP"]},{"title":"[Doc] Hexo Manual","url":"/2022/05/21/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo new <span class=\"token string\">\"My New Post\"</span>\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo server\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo generate\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo deploy\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","categories":["Documentations"]}]